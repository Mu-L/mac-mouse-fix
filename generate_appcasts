#!/usr/bin/python3

# Imports

import sys
import os
import shutil
# import requests # Only ships with python2 on mac it seems. Edit: So what? We could just install it right?
import urllib.request
import urllib.parse

from pathlib import Path
import json
import textwrap
from pprint import pprint
import subprocess
import re
import anthropic
import argparse
import hashlib
from typing import Callable
import tempfile
from mfutils import deptracked
from mfutils import mfdedent
from mfutils import HumanBytes
from dataclasses import dataclass

from babel import dates as bdates
import datetime

"""

TODO: [Mar 2025]
    - [x] Integrate this with our scripts repo
        - [ ] Use it to validate the locales
            - We can enforce that this repo should be in a folder like mac-mouse-fix-update-feed. The mac-mouse-fix repo, on the master branch, should be a in a sibling folder – Then we can read the locales from the xcode project.
        - [ ] Replace subprocess and os.system calls with our 'safe' (non-shell) custom implementation. (Now that we're passing LLM output, shell=True isn't safe anymore I think.)
    - [x] For each locale, create folder of localized 'releases' md docs which mirror the GitHub releases page (including asset-download-links)
    - [ ] Rename from generate_appcasts to something that reflects that it also creates 'release' documents now.
    - Anthropic API: 
        - Look into Batch Processing:   https://docs.anthropic.com/en/docs/build-with-claude/batch-processing
        - Look into Prompt Caching:     https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching
        - Optimize Prompt / do 'prompt engineerig' (?)
        - Optimize Model choice (?)
        - Optimize Cost (?)
            - I'll only run this once in a while, so it probably doesn't matter.
        - See if there are other API params to optimize (?)
        - Optimize latency (?)
            - Not that important but current latency of like 30 s per request is annoying to work with. [Mar 2025]
    
    Update: (late [Mar 2025])
        - [ ] Actually create the appcasts.xml file in such a way that it references all the new translated update-notes html files.
        - [ ] Add locale picker at top of release documents – reuse code for for creating locale picker when compiling human-translated markdown documents
        - [ ] Cleanup file/folder structure for all the new files we're creating now.
        - [x] Cleanup comments and stuff maybe
        - [ ] Implement system to link between release-note documents while preserverving language choice.
        - [ ] Perhaps implement 'dynamic glossary' idea which we described in a comment somewhere inside generate_appcasts.
        - [ ] Document/think about problem I just noticed: The refund-email-link we included in several recent release notes is not translatable
            with the systems we've planned.
            I think it's the only user-facing string currently hosted in the redirection service.
            ... Thought: I think like 1 person ever sent us a refund request via that link (?) Perhaps shouldn't spend too much energy on this.

Protocol:  
  - (early) [Mar 2025]
        I machine translated 3.0.3 update notes using the following parameters:
            model:          claude-3-5-sonnet-20241022
            temperature:    0
            system: 
                You are an accurate, elegant translator. Requests that you handle follow the pattern:
                `language_code: <some ISO language code>\\nenglish_text: <some english text>`
                You will reply with the translation of the english text into the language specified by the language code. You do not reply with any other information. Only the translation.
                Context: The text you translate is written by the developer of an indie app. The text is intended to be read by users of said indie app. To be appropriate for this context, the text should not be overly formal. For example, when translating to German, use informal 'du' instead of formal 'Sie'.
            input: 
                f"language_code: {lcode}\nenglish_text: {release_notes_cleaned}"
                On {lcode}:
                    - I fed it 29 language_code's, extracted from the 'knownRegions' in the MMF Xcode project. (from the feature-strings-catalog branch)
                On {release_notes_cleaned}:
                    - Release notes were downloaded straight from the GitHub releases API, I then also stripped out HTML comments since those don't need to be translated.

        I then asked Claude 3.7 (with extended thinking) to thoroughly review the translations.
            - It said they were of excellent quality.
            - It noted that Korean, Japanese, and French use polite forms, which German uses informal 'du'. 
                - I asked it whether that makes sense and it explained that this culturally appropriate and matches trends seen in tech documentation and communication by tech companies.

        I also personally reviewed the German versions:
            - The translations sound very good and natural. Some stuff sounds weird, but I think that's mostly cause my original English texts aren't super well written. (I felt it was very apologetic and verbose in some places. I just felt like who caressss when reading it. But that's not the LLM-translator's fault.)

        -> It seems like this setup produces very good translations.
            (We haven't iterated on the prompt or other parameters, but I think we don't have to since this is good enough.)

        Problems: 
        1. When we use very specific terms mirroring UI elements in the app, that is unlikely to get accurately translated.
            - Solution ideas:
                1. Manual glossary: 
                    Have human translators of the main app create a glossary somehow
                2. Disclaimer: 
                    Mark the update notes as 'machine translated' and 'possibly containing some inaccuracies'
                    -> Other reason to do this: I think people generally appreciate knowing whether the content they look at is AI generated.
                3. Flood LLM context: 
                    Feed all the (human generated) translations from the app and the website into the LLM's context window
                    - Problems: 
                        - Bigger load on the API (Should probably use 'prompt caching'.)
                        - Might degrade the quality if we clutter up the context window
                        - Not sure how handle older UI elements get removed? Then we can't easily update older notes anymore I think. I guess we could feed it different versions of the human generated translations but then we won't be able to employ prompt caching I think and it would complicated everything.
                        - For a lot of the strings in the main MMF app, the primary context for the translators stems from the autogenerated screenshots with markers for where the specific string appears in the screenshot - Might not be feasible to feed this context to the LLM.
                        -> Even with these problems, feeding the human translations as context might still bring worth-it quality improvements.
                4. 'Dynamic glossary':
                    - In this script we'd maintain a 'glossary_keys' list and then we'd extract the English original + translations for those keys from the .xcstrings files 
                      in the 'mac-mouse-fix' repo and paste them into the LLM's context. 
                    -> This should be easy to maintain and make translations quality really good.
                    - Problems:
                        - When older strings referenced by the glossary_keys get removed, we might need special handling -> Could probably just hardcode the latest commit where a specific string is available.

                -> Conclusion: Start with 2. (Disclaimer) and perhaps add 4. (Dynamic glossary) later to improve quality.

    - (late) [Mar 2025]: 
            Updated the prompt with a localizer_hint, plus XML markup:
                System Prompt: 
                    You are an accurate, elegant translator. Requests that you handle follow the pattern:
                    `<language_code>[some ISO language code]</language_code><localizer_hint>[some hint for you]</localizer_hint><english_text>[some english text]</english_text>`
                    You will reply with the translation of the english text into the language specified by the language code. You do not reply with any other information. Only the translation.
                    The localizer hint will either provide additional context to help you with your translation, or it will be '<None>', in which case you can ignore it.
                    Please note: The text you translate is written by the developer of an indie app. The text is intended to be read by users of said indie app. To be appropriate for this context, the text should not be overly formal. For example, when translating to German, use informal 'du' instead of formal 'Sie'.
                    rf"<language_code>{language_code}</language_code><localizer_hint>{localizer_hint}</localizer_hint><english_text>{english_text}</english_text>"
                User Prompt:
                    rf"<language_code>{language_code}</language_code><localizer_hint>{localizer_hint}</localizer_hint><english_text>{english_text}</english_text>"
            
            Reasoning behind these changes:
                - The localizer_hint is necessary to accurately translate very short custom strings like 'Assets'
                - The XML makes things unambiguous if the localizer hint contains `\n` (which was the separator before)
            
            Other stuff I did:
                - I tried twiddling with the prompt a bit but didn't make it better
                - I used the Anthropic console 'improve prompt' feature. I told it to make the translations more natural. It wrote a very long prompt. But I think the results were less natural if anything.
                -> I think our prompt already produces close-to-optimal results
                    

    
"""

# Validate location
assert os.path.basename(os.getcwd()) == 'mac-mouse-fix-update-feed', f"Running from unexpected folder {os.getcwd()}. Explanation: [Mar 2025] This script currently lives in the update-feed branch of the main mac-mouse-fix repo, but the script should be run from a separate folder next to the main repo folder, so it can access the Mac Mouse Fix source code."

# Parse args
argparser = argparse.ArgumentParser("")
argparser.add_argument('--test-mode', action=argparse._StoreTrueAction)
argparser.add_argument('--anthropic-api-key', default=os.environ.get("ANTHROPIC_API_KEY"))
args, unrecognized_args = argparser.parse_known_args()
if unrecognized_args:
    print(f"Ignoring unrecognized args: {unrecognized_args}")

# Test mode
TEST_MODE = False
if len(sys.argv) >= 2 and sys.argv[1] == '--test-mode':
    print('Running in --test-mode')
    TEST_MODE = True

# Constants
#   Paths are relative to project root or to each other.

# URLs
releases_api_url        = "https://api.github.com/repos/noah-nuebling/mac-mouse-fix/releases"

if TEST_MODE:
    base_url            = "http://127.0.0.1:8000"       # For testing. Run `python3 -m http.server 8000` to serve this repo at that url. Background: `file://` and `localhost:` URLs are forbidden by Sparkle, this is a workaround. Read more in README.md. Stand [Feb 2025]
else:
    base_url            = "https://raw.githubusercontent.com/noah-nuebling/mac-mouse-fix/update-feed"

if False:
    proxy_url               = "https://noah-nuebling.github.io/mmf-update-notes-proxy/?url="
    raw_github_url          = "https://raw.githubusercontent.com/noah-nuebling/mac-mouse-fix/master"

# Output paths/folders
appcast_file_name               = "appcast.xml"                             # Path to the appcast for stable releases
appcast_pre_file_name           = "appcast-pre.xml"                         # Path to the appcast for prereleases

appcast_url                     = f"{base_url}/{appcast_file_name}"         # This gets included as a link in appcast.xml. Not sure what it does.
appcast_pre_url                 = f"{base_url}/{appcast_pre_file_name}"

github_releases_folder          = "docs/github-releases/"                         # This where translations of GitHub releases (md) go 
update_notes_folder_html        = "docs/update-notes/html"                        # This is where the html update notes go. They are generated from the .md update notes downloaded off GitHub. [Mar 2025] appcast.xml will reference these html update notes via <sparkle:releaseNotesLink>, so these are displayed directly to the user in the Sparkle update window.
css_file_path                   = "docs/update-notes/style.css"                   # The css file referenced by the html update notes.
js_file_path                    = "docs/update-notes/script.js"                  

# Implementation detail paths/folders
custom_strings_folder           = "docs/intermediates/custom-strings/"
update_notes_folder_markdown    = "docs/intermediates/update-notes-md"              # This is where the raw md update notes go. The English versions are extracted straight from GitHub releases, and the translations are automatically generated by this script. [Mar 2025] These serve as a cache so we can avoid (slowly) re-translating unless necessary.
deptracker_path                 = "docs/dependency_tracker.json"
sparkle_project_path            = "Frameworks/Sparkle-1.27.3"                       # Need this to use Sparkles code-signing tool # This is dangerously hardcoded # Might be smart to keep this in sync with the Sparkle version in the app.
download_folder                 = "generate_appcasts_downloads"                     # This is were we download old app versions to, and then unzip them. We want to delete this on exit. (Update: [Mar 2025] also using this to store other temporary data now)
app_bundle_name                 = "Mac Mouse Fix.app"                               # This is the name of the app bundle after unzipping it
html_header_includes_tmp_file   = f"{download_folder}/html_header_includes.html"    # Temp file for storing html headers for update notes. I guess we're using downloads_folder as a general 'tmp' folder here.

info_plist_app_subpath          = "Contents/Info.plist"                     # Where to look fo the Info.plist file within the unzipped app bundle

prefpane_bundle_name            = "Mouse Fix.prefpane"                       # App has been renamed, this is the old name

# Dynamic paths/folders
current_directory = os.getcwd()
download_folder_absolute = os.path.join(current_directory, download_folder)

# GitHub date parsing
def ui_string_from_gh_date(locale: str, gh_date: str) -> str:
    # Doesn't output hour, minute, second. Only year, month, day
    #   Not moving this into mfutils, since I think it's pretty specific for what we're doing here.
    gh_api_time_format = "%Y-%m-%dT%H:%M:%SZ"
    dt = datetime.datetime.strptime(gh_date, gh_api_time_format)
    dt = dt.replace(tzinfo=datetime.timezone.utc)                   # Set the timezone to +0 (Implied by the 'Z' in the original ISO string from the gh API.)
    result = bdates.format_date(dt, locale=locale)
    return result

# Language codes
#   ISO codes for the languages to translate the update notes into. 
#   Keep in-sync with 'knownRegions' in project.pbxproject of mac-mouse-fix and mac-mouse-fix-website repos:
#       https://github.com/noah-nuebling/mac-mouse-fix/blob/b3c2cb85f81e8637d51b533a43bbc99084e85e89/Mouse%20Fix.xcodeproj/project.pbxproj#L7954
#   (TODO: Perhaps automatically sync the languages with the mac-mouse-fix-website repo – I think it's ok to expect to find that as a sibling folder to the one where this script runs, just like we do for the strings-upload script [Mar 2025])

source_language_code = 'en'

language_codes = [
    'en',
    'de',
    "zh-Hant",
    "zh-HK",
    "zh-Hans",
    'ko',
    'vi',
    'ar',
    'ca',
    'cs',
    'nl',
    'fr',
    'el',
    'he',
    'hu',
    'it',
    'ja',
    'pl',
    "pt-BR",
    "pt-PT",
    'ro',
    'ru',
    'es',
    'sv',
    'tr',
    'uk',
    'th',
    'id',
    'hi',
]

# Stuff for reading directly from the project source files. 
#   We went over to downloading and unzipping all old bundles instead.
#   Note: 
#       Accessing Xcode environment variables is night impossible it seems
#       The only way to do it I found is described here:
#         https://stackoverflow.com/questions/6523655/how-do-you-access-xcode-environment-and-build-variables-from-an-external-script
#         And that's not feasible to do for old versions.

if False:
    info_plist_path = "App/SupportFiles/Info.plist"
    base_xcconfig_path = "xcconfig/Base.xcconfig"
    files_to_checkout = [info_plist_path, base_xcconfig_path]

#
# Main function
#

def generate():

    try:

        # Check if there are uncommited changes
        #   Note: This script uses git stash several times, so they'd be lost Update: [Feb 2025] We don't seem to be using git stash anymore. We can probably turn this off. (./update still checks for uncommited changes, so that should be safe.)
        if TEST_MODE or True:
            pass
        else: 
            uncommitted_changes = subprocess.check_output('git diff-index HEAD --', shell=True).decode('utf-8')
            if (len(uncommitted_changes) != 0):
                raise Exception('There are uncommited changes. Please commit or stash them before running this script.')

        # Main logic
        
        # Call GH API
        with urllib.request.urlopen(releases_api_url) as request:
            releases = json.load(request)

        # Make downloads folder
        os.makedirs(download_folder_absolute, exist_ok=True)

        # Prepare text to include in the html header of all release notes.
        #   (We have to write this to a file because I don't know how else to pass this to pandoc.)
        
        if True: 
            # Approach 1: Reference the css/js files
            #   This only works through githack, because raw.githubusercontent serves the css and js file with text/plain mime type.
            #   
            #   Meta: Is this better than embedding the css/js files directly? 
            #       Contra: The client will have to download the css/js either way to correctly display the update notes. So that's not better.
            #       Pro: The html files inside update-notes/html will be smaller, since they don't all contain a copy of our css/js.
            #       Pro: The *content* in the html files can be updated and diff'd independently of the js/css. This might make it easier to programmatically check for content changes, which might be useful once we translate the update-notes with the help of an LLM but don't wanna regenerate translations if the content didn't change.
            #       Contra: This might **slow down** loading of update notes because we need to download 3 different files through githack. 
            #           But based on [Feb 2025] testing, the slow down is not noticable. Specifically, I saw that: 1. BetterDisplay, another Sparkle app also take a bit to load the notes 2. When you load a note in the browser, it's instant, despite css and js being served through githack. -> So I don't feel like githack makes a difference.
            
            html_header_includes = mfdedent("""
                <link rel="stylesheet" href="{css_slot}"/>
                <script src="{js_slot}"></script>
            """).format(
                css_slot = apply_githack(f'{base_url}/{css_file_path}'),
                js_slot = apply_githack(f'{base_url}/{js_file_path}')
            )
        else: 
            # Approach 2: include the css/js files directly
            html_header_includes = mfdedent("""
                <style> 
                {css_slot} 
                </style>
                <script> 
                {js_slot} 
                </script>
            """).format(css_slot = textwrap.indent(Path(css_file_path).read_text(), 4 * ' '), 
                        js_slot = textwrap.indent(Path(js_file_path).read_text(), 4 * ' '))
        
        with open(html_header_includes_tmp_file, 'w') as f:
            f.write(html_header_includes)

        # We'll be iterating over all releases and collecting data to put into the appcast
        appcast_items = []
        appcast_pre_items = [] # Items for the pre-release channel

        for r in releases:

            # Get short version
            short_version = r['name']
        
            # Log
            print(f'Processing release {short_version}...')

            # Get tag name
            tag_name = r['tag_name']
            
            # Get link to main GitHub releases page
            html_url = r['html_url']

            # Get release notes
            release_notes = r['body'] # This is markdown
            
            # Get publishing date
            publishing_date = r['published_at']

            # Get isPrerelease
            is_prerelease = r['prerelease']

            # Get path to cached English release notes
            md_path_src = os.path.join(update_notes_folder_markdown, source_language_code, tag_name + ".md")

            # Remove HTML comments from release notes
            #   Since those don't have to be translated (And would cost a bunch of extra Claude tokens)
            release_notes = re.sub(r"<!--.*?-->", "", release_notes, count=0, flags=re.DOTALL)
            
            # Write the fresh source-language release notes to file
            p = Path(md_path_src)
            p.parent.mkdir(parents=True, exist_ok=True)
            p.write_text(release_notes)

            # Update translations for release notes
            for lcode in language_codes:
                translation_path = os.path.join(update_notes_folder_markdown, lcode, tag_name + ".md")
                _ = upget_translation(release_notes, lcode, md_path_src, translation_path)

            # Update HTML release notes
            for lcode in language_codes:
                
                # Get dependency paths
                md_path: str            = os.path.join(update_notes_folder_markdown, lcode, tag_name + '.md')
                disclaimer_path: str    = get_custom_string_path(lcode, 'release-notes.disclaimer')

                # Get target path
                html_path: str          = os.path.join(update_notes_folder_html, lcode, tag_name + '.html')

                # Combine dependency paths
                source_paths = None
                if lcode != source_language_code:   source_paths = [disclaimer_path, md_path]
                else:                               source_paths = [md_path]
                
                # Update dependencies (only disclaimer as of [Mar 2025])
                #   Notes: 
                #   - [Mar 2025] This will update the file at disclaimer_path, so gotta call it before the @deptracked updater below
                #   - [Mar 2025] If you change the html_url, the dependency-tracker won't understand that. Delete html docs to force recompute.
                disclaimer = upget_custom_string(lcode, 'release-notes.disclaimer').format(gh_releases_url=html_url) 

                # Convert to HTML
                #   Note: [Mar 2025] Combining stuff into the HTML update-notes should be pretty fast - do we even need to use @deptracker? (We built deptracker to avoid pinging the slow LLM API repeatedly, which we don't gotta do here.)
                @deptracked(deptracker_path, source_paths, html_path)
                def upget():
                    
                    # Combine dependencies
                    combined_md = Path(md_path).read_text()
                    if lcode != source_language_code:
                        combined_md = disclaimer + '\n\n---\n\n' + combined_md

                    # Write combined md to temp file
                    #   Notes: 
                    #       - [Mar 2025] Gotta write to file, otherwise we couldn't escape it properly for pandoc IIRC.
                    #       - [Mar 2025] Not sure we should use tempfile or our download_folder for this temp file
                    md_path_temp = tempfile.NamedTemporaryFile().name
                    Path(md_path_temp).write_text(combined_md)
                    print(f"Wrote combined markdown to '{md_path_temp}'")

                    # Convert combined md release notes to HTML 
                    release_notes_html = subprocess.check_output(
                        f"cat {md_path_temp} | "
                        "pandoc "
                        "--from markdown --to html "
                        "--standalone "                 # Not sure what this does / if it's necessary
                        f"--include-in-header ./{html_header_includes_tmp_file} "
                        "--metadata title='' "
                        "--metadata document-css=false" # Stops pandoc from adding some of its default inline css, but can't manage to turn that off entirely.
                        ,
                        shell=True
                    ).decode('utf-8')

                    # Return
                    return release_notes_html
                _ = upget()

            # Update Release documents
            #   (Create md docs mirroring GitHub releases pages (but localized)
            for lcode in language_codes:
                
                # Skip English
                #   (The English 'release document' is just the actual GitHub Release page)
                if lcode == source_language_code:
                    continue

                # Get dependency paths
                md_path = os.path.join(update_notes_folder_markdown, lcode, tag_name + ".md")
                kdisclaimer = 'release.disclaimer'
                kmetadata   = 'release.metadata'
                kassets     = 'release.assets'
                source_paths = [
                    md_path,
                    get_custom_string_path(lcode, kdisclaimer),
                    get_custom_string_path(lcode, kmetadata),
                    get_custom_string_path(lcode, kassets),
                ]

                # Skip if source file doesn't exist
                if not os.path.exists(md_path):
                    print(f"Not creating release document for release notes at '{md_path}' because the file doesn't exist.") # This is to let us skip certain translations during development. Shouldn't happen in production.
                    continue

                # Get target path
                release_doc_path = os.path.join(github_releases_folder, lcode, tag_name + '.md')

                # Update dependencies
                disclaimer      = upget_custom_string(lcode, kdisclaimer)
                metadata        = upget_custom_string(lcode, kmetadata)
                assets_title    = upget_custom_string(lcode, kassets)

                # Apply formatting
                disclaimer = disclaimer.format(gh_releases_url=html_url)
                metadata = metadata.format(date=ui_string_from_gh_date(lcode, publishing_date))

                # Combine dependencies into target
                @deptracked(deptracker_path, source_paths, release_doc_path)
                def upget() -> str:

                    # Build prefix
                    prefix = mfdedent(r"""
                        {language_picker}
                        <table align=><td>
                        {disclaimer}
                        </td></table>

                        <table></table>

                        # {version_name}
                        *{metadata}*

                        <br>
                        """).format(language_picker="[langpick go here]", disclaimer=disclaimer, version_name=short_version, metadata=metadata)
                    
                    # Get strings describing each asset
                    #   [Mar 2025] This is not localized, so we could cache it for each lang.
                    assets_content = ""
                    for i, asset in enumerate(r['assets']):

                        url  = asset['browser_download_url']
                        name = asset['name']
                        size = HumanBytes.format(int(asset['size']), metric=True, precision=1) # This is in metric (1000 B/KB), while on official GH release page it seems to be in binary (1024 B/KB) – but I don't think it'll matter to anyone.

                        if i != 0: 
                            assets_content += '\n'
                        assets_content += mfdedent(r"""
                            <tr>
                                <td><a href="{download_url}">{asset_name}</a></td>
                                <td>{asset_size}</td>
                            </tr>
                            """).format(download_url=url, asset_name=name, asset_size=size)

                    # Build suffix
                    suffix = mfdedent(r"""      
                        ---
                                      
                        <table align="start">
                        <tr>
                            <td colspan=2>
                                <b>{assets_title}</b>
                            </td>
                        </tr>
                        {assets_content}
                        </table>
                        """).format(assets_title=assets_title, assets_content=assets_content)
                    
                    # Get release notes
                    release_notes = Path(md_path).read_text()

                    # Combine
                    result = prefix + '\n\n' + release_notes + '\n\n' + suffix

                    # Return
                    return result
                upget()
            
            # Get title
            title = f"{short_version} available!"

            # Get type
            type = "application/octet-stream" # Not sure what this is or if this is right

            if False:

                # Tried to checkout each commit and then read bundle version and minimum compatible macOS version from the local Xcode source files. 
                # I had trouble making this approach work, though, so we went over to just unzipping each update and reading that data directly from the bundle

                # Get commit number
                # commit = os_system_exc(f"git rev-list -n 1 {tag_name}") # Can't capture the output of this for some reason
                commit_number = subprocess.check_output(f"git rev-list -n 1 {tag_name}", shell=True).decode('utf-8')
                commit_number = commit_number[0:-1] # There's a linebreak at the end

                # Check out commit
                # This would probably be a lot faster if we only checked out the files we need
                os_system_exc("git stash")
                files_string = ' '.join(files_to_checkout)
                bash_string = f"git checkout {commit_number} {files_string}"
                try:
                    subprocess.check_output(bash_string)
                except Exception as e:
                    print(f"Exception while checking out commit {commit_number} ({short_version}): {e}. Skipping this release.")
                    continue

                # Get version
                #   Get from Info.plist file
                bundle_version = subprocess.check_output(f"/usr/libexec/PlistBuddy {info_plist_path} -c 'Print CFBundleVersion'", shell=True).decode('utf-8')

                # Get minimum macOS version
                #   The environment variable buried deep within project.pbxproj. No practical way to get at this
                #   Instead, we're going to hardcode this for old versions and define a new env variable via xcconfig we can reference here for newer verisons
                #   See how alt-tab-macos did it here: https://github.com/lwouis/alt-tab-macos/blob/master/config/base.xcconfig
                minimum_macos_version = ""
                try:
                    minimum_macos_version = subprocess.check_output(f"awk -F ' = ' '/MACOSX_DEPLOYMENT_TARGET/ {{ print $2; }}' < {base_xcconfig_path}", shell=True).decode('utf-8')
                    minimum_macos_version = minimum_macos_version[0:-1] # Remove trailing \n character
                except:
                    minimum_macos_version = 10.11

            # Get app asset
            # NOTE: This has a copy in stats_internal.py. Keep them in sync.
            app_assets = [asset for asset in r['assets'] if asset['name'] == 'MacMouseFixApp.zip' or asset['name'] == 'MacMouseFix.zip'] # I don't think we need `MacMouseFix.zip` here (That's the old prefpane name.)
            assert len(app_assets) <= 1, f"Found {len(app_assets)} app assets. Here are the asset names: { list(map(lambda a: a['name'], r['assets'])) }"
            if len(app_assets) == 0:
                print(f"Couldn't find asset with standard name. Falling back to first asset, named {r['assets'][0]['name']}")
                app_assets = [r['assets'][0]]
            
            # Get download link
            download_link = app_assets[0]['browser_download_url']

            # Download update
            download_name = download_link.rsplit('/', 1)[-1]
            download_zip_path = f'{download_folder}/{download_name}'
            urllib.request.urlretrieve(download_link, download_zip_path)

            # Get edSignature
            signature_and_length = subprocess.check_output(f"./{sparkle_project_path}/bin/sign_update {download_zip_path}", shell=True).decode('utf-8')
            signature_and_length = signature_and_length[0:-1]

            # Unzip update
            os_system_exc(f'ditto -x -k --sequesterRsrc --rsrc "{download_zip_path}" "{download_folder}"') # This works, while subprocess.check_output() doesn't for some reason

            # Find app bundle
            #   Maybe we could just name the unzipped folder instead of guessing here
            #   Well we also use this to determine if the download is a prefpane or an app. There might be better ways to infer this but this should work
            is_prefpane = False
            app_path = f'{download_folder}/{app_bundle_name}'
            if not os.path.exists(app_path):
                app_path = f'{download_folder}/{prefpane_bundle_name}'
                if not os.path.exists(app_path):
                    raise Exception('Unknown bundle name after unzipping')
                else:
                    is_prefpane = True

            if is_prefpane:
                continue

            # Find Info.plist in app bundle
            info_plist_path = f'{app_path}/{info_plist_app_subpath}'

            # Read stuff from Info.plist
            bundle_version = subprocess.check_output(f"/usr/libexec/PlistBuddy '{info_plist_path}' -c 'Print CFBundleVersion'", shell=True).decode('utf-8')
            minimum_macos_version = subprocess.check_output(f"/usr/libexec/PlistBuddy '{info_plist_path}' -c 'Print LSMinimumSystemVersion'", shell=True).decode('utf-8')
            bundle_version = bundle_version[0:-1]
            minimum_macos_version = minimum_macos_version[0:-1]

            # Delete bundle we just processed so that we won't accidentally process it again next round (that happens if the next bundle has prefpane_bundle_name instead of app_bundle_name)
            shutil.rmtree(app_path)

            # Assemble collected data into appcast.xml-ready item-string
            #  
            #   About release notes & appcast.xml format:
            #       Release notes can be embedded directly in the appcast.xml using <description> or via a link using <sparkle:releaseNotesLink>
            #           Originally, we used <description> to keep things simple, but this caused the appcast.xml to contain countless copies of our custom css and js text.
            #           I couldn't figure out how to fix that without switching to <sparkle:releaseNotesLink>, so we did switch in [Feb 2025]
            #       
            #   <sparkle:releaseNotesLink> complications:
            #   - [Feb 2025] We're prepending base_url, because I don't think the <sparkle:releaseNotesLink> can be a relative URL. 
            #   - [Feb 2025] <sparkle:releaseNotesLink> requires githack since raw.githubusercontent serves the html file with text/plain mime type, which makes Sparkle update window in MMF display the html source code as plain text.
            #       - Before githack, we used a custom proxy to change mime-types, but it worked clientside and so required js which wasn't activated in the update window of older MMF versions.
            #       - See: https://github.com/noah-nuebling/mmf-update-notes-proxy

            #   References: 
            #       - [SUAppcastItem releaseNotesURL] docs (https://sparkle-project.org/documentation/api-reference/Classes/SUAppcastItem.html#/c:objc(cs)SUAppcastItem(py)releaseNotesURL)
            #           - Explains difference between <sparkle:releaseNotesLink> and <description>
            #       - SampleAppcast.xml 1 (https://github.com/sparkle-project/Sparkle/blob/2.x/Resources/SampleAppcast.xml)
            #           - Contained in main Sparkle repo, uses <sparkle:releaseNotesLink>
            #       - SampleAppcast.xml 2 (https://sparkle-project.org/files/sparkletestcast.xml)
            #           - Linked from Sparkle docs, uses <description>

            if True:
                # Approach 1: <sparkle:releaseNotesLink>
                #   (Requires githack to fix mime type)
                release_notes_str = "<sparkle:releaseNotesLink>{release_notes_link_slot}</sparkle:releaseNotesLink>".format(
                    release_notes_link_slot = apply_githack(f"{base_url}/{html_path}")
                )
            else:
                # Approach 2: <description>
                #   (This bloats the appcast file quite a lot)
                release_notes_str = mfdedent("""
                    <description>
                    {release_notes_slot}
                    </description>
                """).format(release_notes_slot = release_notes_html)

            item_string = mfdedent("""
                <item>
                    <title>{title_slot}</title>
                    <pubDate>{publishing_date_slot}</pubDate>
                    <sparkle:minimumSystemVersion>{minimum_macos_version_slot}</sparkle:minimumSystemVersion>
                    {release_notes_str_slot}
                    <enclosure
                        url=\"{download_link_slot}\"
                        sparkle:version=\"{bundle_version_slot}\"
                        sparkle:shortVersionString=\"{short_version_slot}\"
                        {signature_and_length_slot}
                        type=\"{type_slot}\"
                    />
                </item>
            """).format(
                title_slot                  = title,
                publishing_date_slot        = publishing_date,
                minimum_macos_version_slot  = minimum_macos_version,
                release_notes_str_slot      = release_notes_str,
                download_link_slot          = download_link,
                bundle_version_slot         = bundle_version,
                short_version_slot          = short_version,
                signature_and_length_slot   = signature_and_length,
                type_slot                   = type
            )

            # Append item_string to arrays
            if not is_prerelease:
                appcast_items.append(item_string)

            appcast_pre_items.append(item_string)

        # Assemble item strings into final appcast strings
        appcast_content_string = mfdedent('''
            <?xml version="1.0" encoding="utf-8"?>
            <rss version="2.0" xmlns:sparkle="http://www.andymatuschak.org/xml-namespaces/sparkle"  xmlns:dc="http://purl.org/dc/elements/1.1/">
            <channel>
                <title>Mac Mouse Fix Update Feed</title>
                <link>{}</link>
                <description>Stable releases of Mac Mouse Fix</description>
                <language>en</language>
                {}
            </channel>
            </rss>
        ''').format(appcast_url, '\n'.join(appcast_items))

        appcast_pre_content_string = mfdedent('''
            <?xml version="1.0" encoding="utf-8"?>
            <rss version="2.0" xmlns:sparkle="http://www.andymatuschak.org/xml-namespaces/sparkle"  xmlns:dc="http://purl.org/dc/elements/1.1/">
            <channel>
                <title>Mac Mouse Fix Update Feed for Prereleases</title>
                <link>{}</link>
                <description>Prereleases of Mac Mouse Fix</description>
                <language>en</language>
                {}
            </channel>
            </rss>
        ''').format(appcast_pre_url, '\n'.join(appcast_pre_items))

        # Write to file
        with open(appcast_file_name, "w") as f:
            f.write(appcast_content_string)
        with open(appcast_pre_file_name, "w") as f:
            f.write(appcast_pre_content_string)

        # Cleanup & exit
        clean_up(download_folder)
        exit(0)

    except Exception as e: # Exit immediately if anything goes wrong
        print(e)
        clean_up(download_folder)
        exit(1)

#
# Other helpers
#

def clean_up(download_folder):
    ret = os.system(f'rm -R {download_folder}')
    if ret != 0:
        print(f'Clean up failed with error code: {ret}')

def apply_githack(url: str):
    # Notes: 
    #   - Based on raw.githack.com, they have very good uptime and have been running for over 10 years (copyright is from 2013) – So I think it's ok to rely on that service.
    #   - raw.githack.com/faq strongly recommends using rawcdn.githack.com in production (instead of raw.githack.com), but that requires us to know the git commit and manually update URLs when content changes (if I understood correctly) – which would complicate the logic here.
    #     Discussion:
    #       Specifically raw.githack.com says: "Please use CDN URLs for anything that might result in heavy traffic. [...] If you don't and the service gets a lots of requests from the same domain, all further requests will be temporary redirected to corresponding CDN URLs"
    #       I assume this is to prevent costs on their side.
    #       I assume traffic from the Sparkle update window in MMF wouldn't trigger the CDN fallback, since the traffic wouldn't come from 'the same domain'. But I'm not sure. If the fallback does trigger, it could delay the rollout of changes we make to update notes. It could also perhaps cause other problems I can't think of right now.
    #       However, I still think it's ok to not use the CDN URL here, since the traffic generated by MMF should be quite low.
    #       I tested with Little Snitch in [Feb 2024] using a newer MMF build (I believe it used Sparkle 1.27): And it seems that githack is only pinged when the update notes are actually showed to the user – When no update is found, then githack is not pinged.
    #       When githack is pinged, it will serve the html, css, and js for the update notes for exactly one update. Looking at the size of those files, this should only be a few KB of traffic.
    #           -> Based on this, I believe the traffic to githack should be pretty miniscule, and therefore I assume it's ok to *not* use the CDN URL.
    #           -> I also set up a 10$-per-month patreon donation for the githack maintainer in [Feb 2025]. I assume that will (more than) offset any costs caused by this.

    if TEST_MODE:
        return url  # In TEST_MODE we host the files locally and they'll be served with the right mime type
    else:
        return url.replace('raw.githubusercontent.com', 'raw.githack.com')

def os_system_exc(s): 
    ret = os.system(s)
    if ret != 0:
        raise Exception(f"os.system failed with error code {ret}")

# Custom strings
# Explanation:
#   [Mar 2025] The main localizable strings this script deals with are the release notes obtained from the GitHub releases API, 
#   but we need some additional localizable strings - those are defined here.
# Notes: 
#   - [Mar 2025] Leading and trailling whitespace seems to be lost after translation with Claude, so we shouldn't include them here.
#   - [Mar 2025] Use upget_custom_string() to access (translated versions) of these. 
#   - On 'release-notes.disclaimer' string:
#       - [Mar 2025] 'not completely accurate' is translated to German as 'nicht vollständig präzise' which is pretty awkward. 'not completely accurate' is translated as 'nicht vollständig korrekt' which is better.
#           -> Perhaps we should tune the LLM params to sound more natural, or alternatively hardcode some strings?

@dataclass
class LocalizableString:
    string: str
    hint: str|None = None

custom_strings_in_src_language: dict[str, LocalizableString] = {
    'release-notes.disclaimer': LocalizableString(mfdedent(r""" 
        **ℹ️ Translated by AI**

        This release note has been translated by the Claude AI. It may not be entirely accurate.\
        For the original English version, [click here]({gh_releases_url}).
        """)),
    'release.disclaimer': LocalizableString(mfdedent(r"""
        <b>Translated by AI</b><br>
        This is a translation of a <b><em>GitHub Release</em></b>.<br>
        The translation was made by the Claude AI and may not be entirely correct.<br>
        For the original GitHub Release (in English), click re</a>.<a href="{gh_releases_url}">he
        """)),
    'release.metadata': LocalizableString(
        r"**Release Date:** {date}",   
        hint="Metadata displayed right below the header of a release document for the Mac Mouse Fix app."),
    'release.assets': LocalizableString(
        r"Assets",                     
        hint="This is a heading for a table showing downloadable files/resources for a release of the Mac Mouse Fix app."),

    # Handwritten German versions (unused, just for reference, as of [Mar 2025])
        '__de.release.disclaimer': LocalizableString(mfdedent(r"""
            <b>Von KI übersetzt</b><br>
            Dies ist eine Übersetzung von einem <b><em>GitHub Release</em></b>.<br>
            Die Übersetzung wurde von der Claude KI erstellt und ist möglicherweise nicht ganz korrekt.<br>
            Das ursprüngliche GitHub Release (auf englisch) findest du <a href="{gh_releases_url}">hier</a>.
            """)),
        '__de.release.metadata': LocalizableString(
            "**Veröffentlichungsdatum:** {date}"),
        '__de.release.assets': LocalizableString(
            "Assets"),
}

def get_custom_string_path(lcode: str, strkey: str) -> str: 
    return os.path.join(custom_strings_folder, lcode, strkey + '.txt')

def upget_custom_string(lcode: str, strkey: str) -> str:

    # Validate args
    assert lcode in language_codes,                     f"Unknown language code {lcode}"
    assert strkey in custom_strings_in_src_language,    f"Unknown string key {strkey}"

    # Derive stuff
    src_path:           str                 = get_custom_string_path(source_language_code, strkey)
    translation_path:   str                 = get_custom_string_path(lcode, strkey)
    fresh_src:          LocalizableString   = custom_strings_in_src_language[strkey]

    # Write the fresh source-language-string to file
    #   Note: [Mar 2025] Is it inefficient to do this every time here? Maybe we could restrict this to only happen once per strkey, per program-run
    p = Path(src_path)
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(fresh_src.string)

    # Do translation
    #   Note: [Mar 2025] We have one file per strkey – is that inefficient or something?
    result = upget_translation(fresh_src, lcode, src_path, translation_path)

    # Return
    return result

#
# Translation helpers
#

def _let_claude_translate(language_code: str, english_textttt: str|LocalizableString) -> str:

    # [Mar 2025] Helper for upget_translation()

    # Preprocess args
    english_text: str = None
    localizer_hint: str = None
    if isinstance(english_textttt, LocalizableString):
        english_text = english_textttt.string
        localizer_hint = english_textttt.hint
    else:
        english_text = english_textttt
        localizer_hint = None
    localizer_hint = localizer_hint or '<None>'

    # Validate
    assert english_text     != '' and english_text is not None, f"English input is empty"
    assert localizer_hint   != '',                              f"Localizer hint is empty but not None"

    # Lazily create anthropic client (Store it on the function object)
    if not hasattr(_let_claude_translate, "anthropic_client"):
        api_key = args.anthropic_api_key
        assert api_key is not None, "Error: No anthropic API key found (Provide it via arg or environment var)."
        _let_claude_translate.anthropic_client = anthropic.Anthropic(api_key=api_key)
        assert _let_claude_translate.anthropic_client is not None, f"Error: No anthropic API client couldn't be created. api_key: {api_key}"
    anthropic_client = _let_claude_translate.anthropic_client

    # Constant: Max output tokens
    #   Discussion: [Mar 2025]
    #   - The `3.0.1 Beta 1` notes have a bit over 1000 tokens.
    #   - 8192 is currently the max. That seems sorta low? But should be enough for our usecase. 
    #   - If we reach the limit, the stop_reason is set to 'max_tokens'
    max_output_tokens = 8192
    
    # Create request for anthropic API
    anthropic_args = {
        'model': "claude-3-5-sonnet-20241022",
        'system': mfdedent(r"""
            You are an accurate, elegant translator. Requests that you handle follow the pattern:
            `<language_code>[some ISO language code]</language_code><localizer_hint>[some hint for you]</localizer_hint><english_text>[some english text]</english_text>`
            You will reply with the translation of the english text into the language specified by the language code. You do not reply with any other information. Only the translation.
            The localizer hint will either provide additional context to help you with your translation, or it will be '<None>', in which case you can ignore it.
            Please note: The text you translate is written by the developer of an indie app. The text is intended to be read by users of said indie app. To be appropriate for this context, the text should not be overly formal. For example, when translating to German, use informal 'du' instead of formal 'Sie'.
            """),
        'messages': [{
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": rf"<language_code>{language_code}</language_code><localizer_hint>{localizer_hint}</localizer_hint><english_text>{english_text}</english_text>"
                }
            ]
        }]
    }

    # Get input token count
    input_token_count_response = anthropic_client.messages.count_tokens(**anthropic_args)
    input_token_count = input_token_count_response.input_tokens
    
    # Log
    print(f"Sending request to anthropic with {input_token_count} input tokens...")

    # Ping Anthropic
    anthropic_args = { 
        **anthropic_args, 
        'max_tokens':   max_output_tokens,
        'temperature':  0,
    }
    response = anthropic_client.messages.create(**anthropic_args)
    
    # Validate response
    assert response.stop_reason == 'end_turn', f"Anthropic model stopped with unexpected reason '{response.stop_reason}'"
    
    # Extract translation text
    translation = response.content[0].text

    # Return
    return translation

def upget_translation(
    fresh_src: str|LocalizableString, 
    translation_language_code: str,
    src_path: str,
    translation_path: str,
) -> str:
    
    # Caution:
    #   [Mar 2025] src_path must be updated to contain fresh_src before this is called, otherwise the dependency tracker will incorrectly reference the outdated content of the source file, while the translation is actually derived from the newer fresh_src.
    #       -> Perhaps we could make this less error-prone (but also less efficient) by not passing in fresh_src, or by writing fresh_src to src_path inside this function. 
    #       Related: Explanation or 'upget' function-prefix explained in the deptracked() implementation

    # Skip source language (English)
    if translation_language_code == source_language_code: 
        return fresh_src.string if isinstance(fresh_src, LocalizableString) else fresh_src

    # Get translation
    @deptracked(deptracker_path, [src_path], translation_path)
    def upget():
        
        # Constants
        user_allowed_translate_all_key = f'user_allowed_translate_all - {src_path}'

        # Check if user allows this translation
        #   (User might wanna avoid expensive/slow API calls.)
        user_allowed_translate = None
        if hasattr(upget_translation, user_allowed_translate_all_key):
            user_allowed_translate = getattr(upget_translation, user_allowed_translate_all_key)
        else:
            while True:
                user_input = input(f"Translate '{src_path}' to language '{translation_language_code}'? (Might incur API costs) [y/n/ya/na - ya/na to apply choice to all remaining languages without asking again.]")
                if   user_input == 'y':     user_allowed_translate = True
                elif user_input == 'n':     user_allowed_translate = False
                elif user_input == 'ya':    user_allowed_translate = True;                          setattr(upget_translation, user_allowed_translate_all_key, True)
                elif user_input == 'na':    user_allowed_translate = False;                         setattr(upget_translation, user_allowed_translate_all_key, False)
                else:                       print(f'Error: Input {user_input} unrecognised.');      continue
                break
        assert user_allowed_translate is not None, "Code is buggy"
        
        if not user_allowed_translate:
            return None # Signal to deptracker decorator that we didn't derive the string (by returning None)
        
        # Log
        print(f"Translating to '{translation_language_code}'")

        # Ask LLM to translate
        translation = _let_claude_translate(translation_language_code, fresh_src)

        # Validate
        assert translation is not None, f"Translation for '{translation_path}' from LLM is unexpectedly None"

        # Return
        return translation
    translation = upget()

    # Fall back to placeholder
    if translation is None: translation = "<No translation>"

    # Return
    return translation

#
# Run
#

generate()